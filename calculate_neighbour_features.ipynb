{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monetary-christopher",
   "metadata": {},
   "source": [
    "Find nearest nodes here and convert to weighted counter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from utils import create_nodes_with_counters, load_preprocessed_counters, load_labels_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name = \"london\"\n",
    "\n",
    "data_dir = Path(\"/Users/martin/PycharmProjects/traffic4cast/data/\")\n",
    "# data_dir = Path(\"traffic4cast/data/\")\n",
    "\n",
    "traffic_path = data_dir / \"traffic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-devices",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_parquet(data_dir / f\"road_graph/{city_name}/road_graph_nodes.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842b46c-9337-4c1e-9c84-27a9f58c695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melbourne False, London True\n",
    "nodes_with_counters = create_nodes_with_counters(city_name, blacklist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-cancer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node pairwise distance matrix D (counter_dim x counter_dim)\n",
    "import time\n",
    "stm = time.time()\n",
    "D = euclidean_distances(nodes_with_counters[[\"x\", \"y\"]])\n",
    "print(f\"Took {time.time() - stm} seconds\")\n",
    "\n",
    "# Takes a bit too long for 90K+ edges\n",
    "# 10k - 1.2s\n",
    "# 20k - 5s\n",
    "# 25k - 8.12s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-centre",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_volume_matrix(counts: pd.DataFrame, feature: str, mode=\"train\"):\n",
    "\n",
    "    if mode == \"train\":\n",
    "        grouper = [\"day\", \"t\"]\n",
    "    else:\n",
    "        grouper = [\"test_idx\"]\n",
    "    \n",
    "    # Create volume matrix V (t x counter_dim)\n",
    "    volume_matrix = counts.pivot(index=grouper, columns=\"node_id\", values=feature)\n",
    "    row_mapping = {k:v for k, v in zip(volume_matrix.index, range(len(volume_matrix)))}\n",
    "    column_mapping = {k:v for k, v in zip(range(volume_matrix.shape[1]), volume_matrix.columns)}\n",
    "    row_mapping_inverse = {v:k for k, v in zip(volume_matrix.index, range(len(volume_matrix)))}\n",
    "    column_mapping_inverse = {v:k for k, v in zip(range(volume_matrix.shape[1]), volume_matrix.columns)}\n",
    "    \n",
    "    # Get ride of nans\n",
    "    vmc = volume_matrix.count()\n",
    "    for col in vmc[vmc < vmc.max()].index:\n",
    "        volume_matrix[col] = volume_matrix[col].ffill().bfill()\n",
    "        \n",
    "    V = volume_matrix.to_numpy()\n",
    "    print(V.shape)\n",
    "    \n",
    "    return {\n",
    "        \"matrix\": V,\n",
    "        \"row_mapping\": row_mapping,\n",
    "        \"column_mapping\": column_mapping,\n",
    "        \"row_mapping_inverse\": row_mapping_inverse,\n",
    "        \"column_mapping_inverse\": column_mapping_inverse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-donor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_softmax_inverse_distance_weighted(D: np.array, k: int, denum_factor: float):\n",
    "    \"\"\"\n",
    "    Create (sparse) weight matrix B (edge_dim x counter_dim).\n",
    "    For simplification, we'll instead map edges to nearest counters, so B becomes (counter_dim x counter_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Take a K column submatrix with the nearest neighbours\n",
    "    argsorted = np.argsort(D, axis=1)[:,:k]\n",
    "    row_index = np.arange(len(D))\n",
    "    nearest = D[row_index[:,None], argsorted]\n",
    "\n",
    "    # Mapping distances to weights\n",
    "#     denum_factor = 0.0005\n",
    "    D_w = (1 / (nearest + denum_factor)) / 1000\n",
    "    print(D_w.shape)\n",
    "    \n",
    "    # Softmax over rows\n",
    "    mx = np.max(D_w, axis=-1, keepdims=True)\n",
    "    numerator = np.exp(D_w - mx)\n",
    "    denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
    "    S = numerator/denominator\n",
    "    \n",
    "    # Finally, create a sparse matrix with only softmax values over K neighbours filled in\n",
    "    B = np.zeros_like(D)\n",
    "    B[row_index[:,None], argsorted] = S\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-light",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_labels():\n",
    "    # For test, we need to generate labels structure ourself\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    test_periods = 100\n",
    "\n",
    "    # For test set, we need to create a submission set of length len(edges) * counters_test[\"test_idx\"].nunique()\n",
    "    # Do this in iterations, as direct join returned weird DF shape\n",
    "    full_test = []\n",
    "    for t in tqdm(range(test_periods)):\n",
    "        full = edges[[\"u\", \"v\"] + engineered_edge_features_to_keep].copy()\n",
    "        full[\"test_idx\"] = t\n",
    "        full_test.append(full)\n",
    "\n",
    "    full_test = pd.concat(full_test)\n",
    "    print(full_test.shape)\n",
    "    return full_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-seafood",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Just nearest neighbour volumes\n",
    "# B_1 = calculate_softmax_inverse_distance_weighted(k=1, denum_factor=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5272aaf-3984-4596-bce3-c7d51e66d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "where_zero = np.where(D == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_30 = calculate_softmax_inverse_distance_weighted(D, k=30, denum_factor=0.0005)\n",
    "np.median(B_30[where_zero[0], where_zero[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_50 = calculate_softmax_inverse_distance_weighted(D, k=50, denum_factor=0.0005)\n",
    "np.median(B_50[where_zero[0], where_zero[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547e0e2-4f54-47ce-a916-084d9bbed64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_100 = calculate_softmax_inverse_distance_weighted(D, k=100, denum_factor=0.0004)\n",
    "np.median(B_100[where_zero[0], where_zero[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e02844-3f65-4a1b-b0c2-09d96dff1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_300 = calculate_softmax_inverse_distance_weighted(D, k=300, denum_factor=0.0003)\n",
    "np.median(B_300[where_zero[0], where_zero[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120342c-88ef-4d91-b179-6709b2caf59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_500 = calculate_softmax_inverse_distance_weighted(D, k=500, denum_factor=0.00025)\n",
    "np.median(B_500[where_zero[0], where_zero[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrary weighting over whole city\n",
    "# The smaller denum_factor, the heavier is weight on closest observations\n",
    "B_city_0008 = calculate_softmax_inverse_distance_weighted(D, k=len(D), denum_factor=0.0008)\n",
    "np.median(B_city_0008[where_zero[0], where_zero[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_city_0005 = calculate_softmax_inverse_distance_weighted(k=len(D), denum_factor=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_city_0002 = calculate_softmax_inverse_distance_weighted(k=len(D), denum_factor=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_city_00015 = calculate_softmax_inverse_distance_weighted(D, k=len(D), denum_factor=0.00015)\n",
    "np.median(B_city_00015[where_zero[0], where_zero[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_city_000125 = calculate_softmax_inverse_distance_weighted(k=len(D), denum_factor=0.000125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_city_0001 = calculate_softmax_inverse_distance_weighted(k=len(D), denum_factor=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-bikini",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_features = {\n",
    "    # \"B_1\": B_1,\n",
    "    # \"B_10\": B_10,\n",
    "    \"B_30\": B_30,\n",
    "    \"B_50\": B_50,\n",
    "    \"B_100\": B_100,\n",
    "    \"B_300\": B_300,\n",
    "    \"B_500\": B_500,\n",
    "    \"B_city_0008\": B_city_0008,\n",
    "    # \"B_city_0005\": B_city_0005,\n",
    "    # \"B_city_0002\": B_city_0002,\n",
    "    \"B_city_00015\": B_city_00015,\n",
    "    # \"B_city_000125\": B_city_000125,\n",
    "    # \"B_city_0001\": B_city_0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-struggle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-protein",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineered_edge_features_to_keep = [\"edge_int\", \"nearest_counter_id\", \"counter_distance_euclidean\", \"counter_distance_euclidean_mean_all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324b51b-0a29-4c87-86fe-161535f6f64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408970ee-ea94-42d2-b19c-254b9ff1d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hacky McHack\n",
    "# sample_counters = {\n",
    "#     \"melbourne\": \"2020-06-01\",\n",
    "#     \"london\": \"2019-07-01\",\n",
    "#     \"madrid\": \"2021-06-01\"\n",
    "# }\n",
    "\n",
    "# # Use one counter slice to find nearest counters\n",
    "# # TODO - annoyingly, the counters seem to be a (slightly) changing set, need to handle this\n",
    "# # TODO calculate superset of all counters, looping over files\n",
    "# counters = pd.read_parquet(data_dir / f\"train/{city_name}/input/counters_{sample_counters[city_name]}.parquet\")\n",
    "# counters = counters[counters.t == 4]\n",
    "# del counters[\"volumes_1h\"]\n",
    "\n",
    "# print(counters[\"node_id\"].nunique())\n",
    "\n",
    "# nodes_with_counters[\"node_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727760d-0fa3-4a33-ac8b-94996d204037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a6197-69c7-493a-9371-e307455a0e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_train = load_preprocessed_counters(city_name, mode=\"train\")\n",
    "print(counts_train[\"node_id\"].nunique())\n",
    "counts_train = counts_train[counts_train[\"node_id\"].isin(nodes_with_counters[\"node_id\"])]\n",
    "print(counts_train[\"node_id\"].nunique())\n",
    "counts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce870ce4-8b6c-4b43-bc4a-c7ad4c4dce3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d65bf-47ee-43e4-90d8-e5fa0513527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_volume_features = [\"volumes_last\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422eb24-e84b-4728-9201-e642a69678fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixes = {}\n",
    "for f in engineered_volume_features:\n",
    "    vol_matrix_output = create_volume_matrix(counts_train, f, \"train\")\n",
    "    V = vol_matrix_output[\"matrix\"]\n",
    "    \n",
    "    # Weighted counter observations W = VB^T (t x counter_dim), where row\n",
    "    # w_{t, i} = \\sum_{j}^{counter_dim} v_{t, j} * b_{i, j}\n",
    "    for w_feat in weighted_features:\n",
    "        print(f, w_feat)\n",
    "        w_mat = weighted_features[w_feat]\n",
    "        matrixes[f\"{w_feat}_{f}\"] = np.dot(V, w_mat).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save volume matrixes\n",
    "with open(traffic_path / city_name / \"volume_matrix.pkl\", \"wb\") as f:\n",
    "    pickle.dump((matrixes, vol_matrix_output[\"row_mapping\"]) , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3205c6-f35d-4769-b87b-433b8205229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b31579-4d5b-4242-b72b-95af70a7089a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6772dd-02dc-4191-92d5-0cf6f1eb4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_values(df: pd.DataFrame, feature_name: str, weight_matrix: np.array, row_mapping: dict, mode=\"train\"):\n",
    "    # There's labels for 2020-06-03 in Melbourne, but not counters :scream:\n",
    "    # Let's keep NaN values for when counter data is missing\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        vals = []\n",
    "\n",
    "        for d, t, c in tqdm(zip(df[\"day\"], df[\"t\"], df[\"nearest_counter_id\"])):\n",
    "            row = row_mapping.get((d, t))\n",
    "            if row is not None:\n",
    "                val = weight_matrix[row,c]\n",
    "            else:\n",
    "                val = np.nan\n",
    "            vals.append(val)\n",
    "    else:\n",
    "        vals = []\n",
    "\n",
    "        for t, c in tqdm(zip(df[\"test_idx\"], df[\"nearest_counter_id\"])):\n",
    "            row = row_mapping.get(t)\n",
    "            if row is not None:\n",
    "                val = weight_matrix[row,c]\n",
    "            else:\n",
    "                val = np.nan\n",
    "            vals.append(val)\n",
    "    \n",
    "    df[f\"euclidean_{feature_name}\"] = vals\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8a98f-0a40-4191-a166-aede0c4af6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64de0a80-e2d6-48b5-a64b-5c1dcae86eab",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9d19d-de1d-430e-9c18-19f8bab1b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_test = load_preprocessed_counters(city_name, mode=\"test\")\n",
    "print(counts_test[\"node_id\"].nunique())\n",
    "counts_test = counts_test[counts_test[\"node_id\"].isin(nodes_with_counters[\"node_id\"])]\n",
    "print(counts_test[\"node_id\"].nunique())\n",
    "counts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7a92a-eb21-4303-868a-bdb82f2a8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixes_test = {}\n",
    "for f in engineered_volume_features:\n",
    "    vol_matrix_output_test = create_volume_matrix(counts_test, f, \"test\")\n",
    "    V_test = vol_matrix_output_test[\"matrix\"]\n",
    "    \n",
    "    # Weighted counter observations W = VB^T (t x counter_dim), where row\n",
    "    # w_{t, i} = \\sum_{j}^{counter_dim} v_{t, j} * b_{i, j}\n",
    "    # if f == \"volumes_gr\":\n",
    "    #     w_feat = \"B_1\" # Save time\n",
    "    #     w_mat = weighted_features[w_feat]\n",
    "    #     matrixes_test[f\"{w_feat}_{f}\"] = np.dot(V, w_mat)\n",
    "    # else:\n",
    "    for w_feat in weighted_features:\n",
    "        print(f, w_feat)\n",
    "        w_mat = weighted_features[w_feat]\n",
    "        matrixes_test[f\"{w_feat}_{f}\"] = np.dot(V_test, w_mat).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702040f4-4387-40c5-a3a3-78d86c658e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save volume matrixes\n",
    "with open(traffic_path / city_name / \"volume_matrix_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump((matrixes_test, vol_matrix_output_test[\"row_mapping\"]) , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f36f2-4a2e-43a1-90b6-257971b4ec71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70bd99-80ab-41ee-9723-db8ce3a8ae35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.r5.8xlarge",
  "kernelspec": {
   "display_name": "t4c22",
   "language": "python",
   "name": "t4c22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
